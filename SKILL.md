---
name: document-intelligence
description: |
  **[REQUIRED]** For **ALL** document/file extraction, parsing, or processing tasks with Snowflake Cortex AI.
  Use when: extracting data from PDFs, parsing documents/files, processing invoices/contracts/reports, 
  analyzing charts/blueprints, building document pipelines, reading files from stage, OCR tasks.
  Triggers: 
  - Documents: document extraction, parse document, documents in stage, docs, paperwork, papers, pages, forms, sheets
  - Files: file extraction, parse file, files in stage, process files, read PDF, attachments, uploads, assets, blobs, objects, artifacts, staged files, uploaded files
  - Formats: PDFs, Word docs, Excel files, spreadsheets, slides, presentations, images, scans, screenshots, scanned documents
  - Business docs: invoices, receipts, bills, statements, quotes, purchase orders, POs, contracts, agreements, policies, claims, reports, manuals, guides, specs, datasheets, applications, certificates, licenses, letters, memos, proposals, work orders, medical scans, lab results, prescriptions, medical records, patient records, healthcare documents, clinical notes, discharge summaries, insurance claims, EOBs, radiology reports, pathology reports
  - Actions: extract from file, extract info, get data from, pull info from, read text from, OCR, digitize, scrape, capture data, analyze these, parse these
  - Functions: AI_EXTRACT, AI_PARSE_DOCUMENT, AI_COMPLETE, document AI
---

# Document Intelligence

Entry point for all document processing tasks using Snowflake Cortex AI.

## Reference Files

This skill uses reference documentation for detailed function guidance:

| Reference | Location | Use For |
|-----------|----------|---------|
| AI_EXTRACT | `reference/ai-extract.md` | Function reference for structured extraction |
| AI_PARSE_DOCUMENT & AI_COMPLETE | `reference/ai-parse-doc-and-ai-complete.md` | Function reference for content parsing |
| **Extraction** | `reference/extraction.md` | Flow A: Structured field/table extraction workflow |
| **Parsing** | `reference/parsing.md` | Flow B: Full content parsing workflow |
| **Visual Analysis** | `reference/visual-analysis.md` | Flow C: Charts, blueprints, diagrams analysis |

**Load sub-skill based on user's extraction goal:**
- For structured fields/tables → Read `reference/extraction.md`
- For full text with layout or OCR → Read `reference/parsing.md`
- For charts, blueprints, diagrams → Read `reference/visual-analysis.md`

## When to Use

- User wants to extract data from documents (PDFs, images, Office files)
- User mentions AI_EXTRACT or AI_PARSE_DOCUMENT
- User wants to process invoices, contracts, forms, reports
- User needs to analyze charts, blueprints, diagrams
- User wants to build document processing pipelines

## Workflow

```
Start
  ↓
Step 1: Gather Document Info
  ↓
  ├─→ Local file → PUT to Snowflake stage
  │
  ├─→ Other storage (GDrive, Dropbox, etc.) → Load openflow skill
  │                                              ↓
  │                                           Setup connector
  │                                              ↓
  │                                           Return here
  ↓
Step 2: Determine Extraction Goal
  ↓
  ├─→ Structured fields/tables → Load reference/extraction.md (Flow A)
  │
  ├─→ Full text with layout → Load reference/parsing.md (Flow B - LAYOUT)
  │
  ├─→ Text via OCR → Load reference/parsing.md (Flow B - OCR)
  │
  └─→ Images/charts/blueprints → Load reference/visual-analysis.md (Flow C)
  ↓
Step 3: Post-Processing Options
  ↓
  ├─→ Store results → Create table / Create pipeline
  │
  └─→ Build RAG/search integration
```

### Step 1: Gather Document Information

**Ask** user about their document using `ask_user_question`:

**Question 1 - File Format:**
```
What type of file do you want to process?
Options:
1. PDF
2. Image (PNG, JPEG, TIFF)
3. Office document (DOCX, PPTX)
4. HTML or TXT file
5. CSV file - AI_EXTRACT only
6. MD or EML file - AI_EXTRACT only
7. Excel file (XLSX, XLS) - not supported
8. Legacy Office (DOC, PPT) - needs conversion
```

**Supported Formats Summary:**
- **AI_EXTRACT supports:** PDF, images (PNG, JPEG, TIFF), DOCX, PPTX, HTML, TXT, CSV, MD, EML
- **AI_PARSE_DOCUMENT supports:** PDF, images (PNG, JPEG, TIFF), DOCX, PPTX, HTML, TXT

**If user selects "CSV file":** CSV files are supported by AI_EXTRACT only (not AI_PARSE_DOCUMENT). Continue to Step 2 to determine extraction goal.

**If user selects "MD or EML file":** MD and EML files are supported by AI_EXTRACT only (not AI_PARSE_DOCUMENT). Continue to Step 2 to determine extraction goal.

**If user selects "HTML or TXT file":** HTML and TXT files are supported by both AI_EXTRACT and AI_PARSE_DOCUMENT. Continue to Step 2 to determine extraction goal.

**If user selects "Legacy Office (DOC, PPT)":**
```
Legacy Office formats (DOC, PPT) are not directly supported.

Please convert to modern formats:
- DOC → DOCX (Save As in Word)
- PPT → PPTX (Save As in PowerPoint)
- Or export as PDF

Then upload the converted file and continue.
```

**If user selects "Excel file (XLSX, XLS)":**

Excel files are not directly supported by AI_EXTRACT or AI_PARSE_DOCUMENT. Inform the user of their options:

```
Excel files (.xlsx, .xls) are not directly supported by the document AI functions.

You have two options:

Option 1: Convert to PDF
- Export the Excel file as PDF from Excel/Google Sheets
- Then upload the PDF and use AI_EXTRACT or AI_PARSE_DOCUMENT

Option 2: Load directly into Snowflake (Recommended for tabular data)
- Snowflake has native Excel support for loading data into tables:

-- Infer schema from Excel file
SELECT * FROM TABLE(
  INFER_SCHEMA(
    LOCATION => '@my_stage/data.xlsx',
    FILE_FORMAT => 'TYPE=EXCEL'
  )
);

-- Query Excel file directly  
SELECT * FROM @my_stage/data.xlsx 
(FILE_FORMAT => 'TYPE=EXCEL', SHEET_NAME => 'Sheet1');

-- Load into table
COPY INTO my_table FROM @my_stage/data.xlsx
FILE_FORMAT = (TYPE = EXCEL)
MATCH_BY_COLUMN_NAME = CASE_INSENSITIVE;

Which option would you like to proceed with?
```

If user chooses Option 1 (convert to PDF), guide them to upload the PDF and continue with the normal flow.

If user chooses Option 2 (load directly), help them with the Snowflake SQL commands above.

**Question 2 - File Location:**
```
Where is your file stored?
Options:
1. Internal Snowflake stage (e.g., @my_db.my_schema.my_stage/file.pdf)
2. External stage (S3, Azure Blob, GCS)
3. Local file on my computer
4. Other storage (Google Drive, Dropbox, SharePoint, OneDrive, Box)
5. Need help setting up a stage
```

**If user selects "Local file on my computer":**

Help user upload file to a Snowflake stage using PUT command.

1. **First, ensure a stage exists** (or create one):
```sql
-- Create a stage if needed (with server-side encryption)
CREATE STAGE IF NOT EXISTS my_db.my_schema.doc_stage
  DIRECTORY = (ENABLE = TRUE)
  ENCRYPTION = (TYPE = 'SNOWFLAKE_SSE')
  COMMENT = 'Stage for document processing';
```

2. **Ask user for file path:**
```
What is the full path to your local file?
Example: /Users/username/Documents/invoice.pdf
         C:\Users\username\Documents\invoice.pdf
```

3. **Generate PUT command** (must be run via SnowSQL, Snowflake CLI, or Python connector):

```sql
-- Using SnowSQL or Snowflake CLI
PUT file:///path/to/your/file.pdf @my_db.my_schema.doc_stage AUTO_COMPRESS=FALSE;
```

**Provide instructions based on user's tool:**

**Option A - SnowSQL:**
```bash
# Install SnowSQL if not installed
# https://docs.snowflake.com/en/user-guide/snowsql-install-config

# Connect and upload
snowsql -a <account> -u <username>
PUT file:///path/to/file.pdf @my_db.my_schema.doc_stage AUTO_COMPRESS=FALSE;
```

**Option B - Snowflake CLI (snow):**
```bash
# Upload using Snowflake CLI
snow stage copy /path/to/file.pdf @my_db.my_schema.doc_stage --overwrite
```

**Option C - Python:**
```python
from snowflake.connector import connect

conn = connect(account='...', user='...', password='...')
cursor = conn.cursor()

# Upload file
cursor.execute("""
    PUT file:///path/to/file.pdf @my_db.my_schema.doc_stage 
    AUTO_COMPRESS=FALSE OVERWRITE=TRUE
""")

print("File uploaded successfully!")
conn.close()
```

**Option D - Multiple files:**
```sql
-- Upload all PDFs from a directory
PUT file:///path/to/folder/*.pdf @my_db.my_schema.doc_stage AUTO_COMPRESS=FALSE;
```

4. **Verify upload:**
```sql
-- List files in stage
LIST @my_db.my_schema.doc_stage;

-- Or using directory table
SELECT * FROM DIRECTORY(@my_db.my_schema.doc_stage);
```

5. **Refresh directory table** (required for DIRECTORY queries):
```sql
ALTER STAGE @my_db.my_schema.doc_stage REFRESH;
```

After file is uploaded, continue with extraction using the stage path.

**If user selects "Other storage":**
- Inform user that files need to be ingested into a Snowflake stage first
- **Load** the `openflow` skill to help set up a connector for their storage provider
- After openflow setup completes, return here to continue with extraction

**Question 3 - Content Type:**
```
What kind of content is in your document?
Options:
1. Structured forms (invoices, receipts, applications)
2. Reports/articles with text and tables
3. Charts, graphs, blueprints, diagrams
4. General text (manuals, policies, contracts)
```

### Step 2: Determine Extraction Goal

**Ask** user what they want to extract:

```
What do you want to extract?
Options:
1. Specific fields (names, dates, amounts, IDs)
2. Tables with defined columns
3. Full text with layout preservation (tables, headers, structure)
4. Text only via OCR (quick text extraction)
5. Images and visual content for analysis (charts, blueprints)
```

**Route based on response:**

| User Selection | Load Sub-Skill | Function |
|----------------|----------------|----------|
| Specific fields, Tables | `reference/extraction.md` | AI_EXTRACT |
| Full text with layout | `reference/parsing.md` | AI_PARSE_DOCUMENT (LAYOUT mode) |
| Text only via OCR | `reference/parsing.md` | AI_PARSE_DOCUMENT (OCR mode) |
| Images, Charts, Blueprints | `reference/visual-analysis.md` | AI_COMPLETE (vision) |

**After routing:** Load the appropriate sub-skill file and follow its workflow.

### Step 3: Post-Processing Options

After extraction/parsing is complete, **ask** user:

```
What would you like to do next?
Options:
1. Done - one-time extraction
2. Store results in a Snowflake table
3. Set up a pipeline for continuous processing
4. Build RAG/search integration
```

**If pipeline setup requested:**
```sql
-- Create stream on stage
CREATE OR REPLACE STREAM doc_stream ON STAGE @my_stage;

-- Create task for continuous processing
CREATE OR REPLACE TASK process_docs
  WAREHOUSE = my_warehouse
  SCHEDULE = '1 MINUTE'
WHEN SYSTEM$STREAM_HAS_DATA('doc_stream')
AS
  INSERT INTO extracted_data
  SELECT ... FROM doc_stream;
```

## Key Constraints

| Constraint | AI_EXTRACT | AI_PARSE_DOCUMENT |
|------------|------------|-------------------|
| Max file size | 100 MB | 50 MB |
| Max pages | 125 | 500 |
| Output format | JSON | Markdown |

## Stopping Points

- After Step 1 if file format is unsupported
- After Step 2 before routing to sub-skill
- After extraction completes for post-processing choice

## Output

Routes user to appropriate sub-skill with gathered context, then assists with post-processing.
